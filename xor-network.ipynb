{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks handcrafted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Dominik Krzemi≈Ñski\n",
    "(26/11/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: teach simple network XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create numerical representation of this simple problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [(0,0), (1,0), (0,1), (1,1)]\n",
    "Y = [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following architecture:\n",
    "![](img/nn2.png)\n",
    "which gives a number of parameters:\n",
    "![](img/nn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our parameter vector according to this. Additionaly, I define $\\eta$ learning rate. We will play around this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 2 * np.random.random(size=(2,3)) - 1\n",
    "w2 = 2 * np.random.random(size=(3,1)) - 1\n",
    "b1 = 2 * np.random.random(size=(1,3)) - 1\n",
    "b2 = 2 * np.random.random(size=(1,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical neuron performs the operation of sum of weighted input and thresholding the activation $a$.\n",
    "\n",
    "![](img/nn4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our activation funtion is sigmoid (you can learn more here: https://en.wikipedia.org/wiki/Sigmoid_function):\n",
    "\n",
    "![](img/nn5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the first derivative as we might need it later.\n",
    "\n",
    "![](img/nn6.png)\n",
    "\n",
    "Yeah, that's a good moment to appreciate simplicity of the above formula. That's one of the reasons why ML researchers like this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y*(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(X[0])\n",
    "y = np.array(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform forward pass first:\n",
    "\n",
    "![](img/nn7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act1 = np.dot(w1.T, x) + b1\n",
    "lay1 = sigmoid(act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act2 = np.dot(w2.T, lay1.T) + b2\n",
    "lay2 = sigmoid(act2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have reached the last layer, we can compute the error of our prediction.\n",
    "![](img/nn9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 0.5 * (y - lay2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we propagate this error backwards, starting from the last layer. We need to use the chain rule to get a formula for the update.\n",
    "\n",
    "For weights it looks like this:\n",
    "\n",
    "![](img/nn10.png)\n",
    "\n",
    "and for bias like this:\n",
    "\n",
    "![](img/nn11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_l2 = (y-lay2) * dsigmoid(act2)\n",
    "corr_w2 = delta_l2 * lay1\n",
    "corr_b2 = delta_l2 * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can update our weights using the gradient descent. Note, how every word makes sense here.\n",
    "- gradient $\\frac{d E}{d w}$\n",
    "- descent \"-\"\n",
    "\n",
    "![](img/nn8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = w2 - eta * corr_w2.T\n",
    "b2 = b2 - eta * corr_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 1 update is a little bit more complicated, as we have to use the chain rule twice:\n",
    "\n",
    "![](img/nn12.png)\n",
    "\n",
    "Try making it by yourself for bias.\n",
    "\n",
    "![](img/nn13.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_l1 = np.dot(w2, delta_l2) * dsigmoid(act1).T\n",
    "corr_w1 = np.outer(x, delta_l1)\n",
    "corr_b1 = delta_l1 * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the parameters, similarly as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1 - eta * corr_w1\n",
    "b1 = b1 - eta * corr_b1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put this together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "# ... or in other words:\n",
    "# how many times we show our network all examples\n",
    "N = 10000 \n",
    "\n",
    "error = np.zeros((N,1))\n",
    "\n",
    "for n in range(N):\n",
    "    for i in range(len(X)): # iterate over all examples\n",
    "        x = np.array(X[i])\n",
    "        y = np.array(Y[i])\n",
    "        # Forward pass, 1st layer\n",
    "        act1 = np.dot(w1.T, x) + b1\n",
    "        lay1 = sigmoid(act1)\n",
    "        # Forward pass, 2nd layer\n",
    "        act2 = np.dot(w2.T, lay1.T) + b2\n",
    "        lay2 = sigmoid(act2)\n",
    "        # Computing error\n",
    "        E = 0.5*(lay2 - y)**2\n",
    "        error[n] += E[0]\n",
    "        # Backprop, 2nd layer\n",
    "        delta_l2 = (lay2-y) * dsigmoid(lay2)\n",
    "        corr_w2 = (delta_l2 * lay1).T\n",
    "        corr_b2 = delta_l2 * 1\n",
    "        # Backprop, 1st layer\n",
    "        delta_l1 = np.dot(w2, delta_l2) * dsigmoid(lay1).T\n",
    "        corr_w1 = np.outer(x, delta_l1)\n",
    "        corr_b1 = (delta_l1 * 1).T\n",
    "        w2 = w2 - eta * corr_w2\n",
    "        b2 = b2 - eta * corr_b2\n",
    "        w1 = w1 - eta * corr_w1\n",
    "        b1 = b1 - eta * corr_b1\n",
    "        if n % 1000 == 0:\n",
    "            print('| {}, {:.3f}'.format(y, lay2[0][0]), end='')\n",
    "    if n % 1000 == 0:\n",
    "        print(' <', '-' * 3, n)\n",
    "    error[n] /= len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N), error)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we implemented so called stochastic gradient descent. We update weights right after seeing a new example.\n",
    "\n",
    "Below we slightly change our code to follow batch gradient descent schema. We update parameters after seeing a batch of examples. In  our case all four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 2 * np.random.random(size=(2,3)) - 1\n",
    "w2 = 2 * np.random.random(size=(3,1)) - 1\n",
    "b1 = 2 * np.random.random(size=(1,3)) - 1\n",
    "b2 = 2 * np.random.random(size=(1,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "error = np.zeros((N,1))\n",
    "for n in range(N):\n",
    "    Dw_1 = np.zeros((2,3))\n",
    "    Dw_2 = np.zeros((3,1))\n",
    "    Db_1 = np.zeros((1,3))\n",
    "    Db_2 = np.zeros((1,1))\n",
    "\n",
    "    for i in range(len(X)): # iterate over all examples\n",
    "        x = np.array(X[i])\n",
    "        y = np.array(Y[i])\n",
    "        # Forward pass, 1st layer\n",
    "        act1 = np.dot(w1.T, x) + b1\n",
    "        lay1 = sigmoid(act1)\n",
    "        # Forward pass, 2nd layer\n",
    "        act2 = np.dot(w2.T, lay1.T) + b2\n",
    "        lay2 = sigmoid(act2)\n",
    "        # Computing error\n",
    "        E = 0.5*(lay2 - y)**2\n",
    "        error[n] += E[0]\n",
    "        # Backprop, 2nd layer\n",
    "        delta_l2 = (lay2-y) * dsigmoid(lay2)\n",
    "        corr_w2 = (delta_l2 * lay1).T\n",
    "        corr_b2 = delta_l2 * 1\n",
    "        # Backprop, 1st layer\n",
    "        delta_l1 = np.dot(w2, delta_l2) * dsigmoid(lay1).T\n",
    "        corr_w1 = np.outer(x, delta_l1)\n",
    "        corr_b1 = (delta_l1 * 1).T\n",
    "        Dw_2 += corr_w2\n",
    "        Dw_1 += corr_w1\n",
    "        Db_2 += corr_b2\n",
    "        Db_1 += corr_b1\n",
    "        if n % 1000 == 0:\n",
    "            print('| {}, {:.3f}'.format(y, lay2[0][0]), end='')\n",
    "    if n % 1000 == 0:\n",
    "        print(' <', '-' * 3, n)\n",
    "    w2 = w2 - eta * Dw_2\n",
    "    b2 = b2 - eta * Db_2\n",
    "    w1 = w1 - eta * Dw_1\n",
    "    b1 = b1 - eta * Db_1\n",
    "    error[n] /= len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N), error)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything you can notice by comparing these two graphs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some people think that RNN are more difficult to understand. Well... conceptually there are some nuances, but implementation is not that difficult.\n",
    "\n",
    "First of all, we need to change the way we think about our problem. Now our $X_r$ is not simply a pair of numbers, but a sequence. We will try to teach our network slightly different problem. To recognise that 1 should follow only two preceding zeros.\n",
    "\n",
    "```\n",
    "0 -> 0 -> 1\n",
    "1 -> 0 -> 0\n",
    "0 -> 1 -> 0\n",
    "1 -> 1 -> 0\n",
    "```\n",
    "\n",
    "Let's try to teach our network that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture looks like this:\n",
    "\n",
    "![](img/nn14.png)\n",
    "\n",
    "Quite similar, isn't it?\n",
    "\n",
    "The only difference is the recurrent layer with parameters $w_h$ and $b_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 2 * np.random.random(size=(1,3)) - 1\n",
    "w2 = 2 * np.random.random(size=(3,1)) - 1\n",
    "b1 = 2 * np.random.random(size=(1,3)) - 1\n",
    "b2 = 2 * np.random.random(size=(1,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = 2 * np.random.random(size=(3,3)) - 1\n",
    "bh = 2 * np.random.random(size=(1,3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the architecture is so similar, it's easy to guess that most of the equations stay the same.\n",
    "\n",
    "Obviously, there is a difference in layer 1:\n",
    "\n",
    "![](img/nn15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rules for $w_1$, $w_2$, $b_1$ and $b_2$ look exactly the same. We need to add update rule for the recurrent hidden layer.\n",
    "\n",
    "![](img/nn16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr = [[0,0,1], [1,1,0], [1,0,0], [0,1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference in adjusting the previous code lays in the time-step propagation. Notice variables `lay1_mem` and `lay2_mem` that contain previous values of layers 1 and 2 respectively (`mem` stands for memory). We need to \"remember\" previous values in order to propagate the error backwards in time. I hope that it agrees with intuitive behaviour of recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "error = np.zeros((N,1))\n",
    "\n",
    "corr_w1 = np.zeros((1,3))\n",
    "corr_b1 = np.zeros((1,3))\n",
    "corr_w2 = np.zeros((3,1))\n",
    "corr_b2 = np.zeros((1,1))\n",
    "corr_wh = np.zeros((3,3))\n",
    "corr_bh = np.zeros((1,3))\n",
    "\n",
    "for n in range(N):\n",
    "    for i in range(len(Xr)):\n",
    "        lay1_mem = [np.zeros((1,3))]  # ! NEW\n",
    "        lay2_mem = []                  # ! NEW\n",
    "        for k in range(2):             # ! NEW\n",
    "            x = np.array(Xr[i][k])\n",
    "            y = np.array(Xr[i][k + 1])\n",
    "            # Forward pass, 1st layer\n",
    "            act1 = np.dot(w1.T, x) + b1.T + (np.dot(lay1_mem[-1], wh) + bh).T # ! NEW\n",
    "            lay1 = sigmoid(act1).T\n",
    "            # Forward pass, 2nd layer\n",
    "            act2 = np.dot(w2.T, lay1.T) + b2\n",
    "            lay2 = sigmoid(act2)\n",
    "            # Computing error\n",
    "            E = 0.5*(lay2 - y)**2\n",
    "            error[n] += E[0]\n",
    "            lay1_mem.append(np.copy(lay1))      # ! NEW\n",
    "            lay2_mem.append(np.copy(lay2))       # ! NEW\n",
    "            if n % 1000 == 0:\n",
    "                print('{} - > {} ({:.3f}|{:.0f})'.format(x, y, lay2[0][0], 1.*(lay2[0][0]>0.5)), end='')\n",
    "        for k in range(1,-1,-1):                 # ! NEW\n",
    "            x = np.array(Xr[i][k])\n",
    "            y = np.array(Xr[i][k + 1])\n",
    "            # Backprop, 2nd layer\n",
    "            delta_l2 = (lay2_mem[k]-y) * dsigmoid(lay2_mem[k])\n",
    "            corr_w2 += (delta_l2 * lay1_mem[k+1]).T\n",
    "            corr_b2 += delta_l2 * 1\n",
    "            # Backprop, 1st layer\n",
    "            delta_l1 = np.dot(w2, delta_l2) * dsigmoid(lay1_mem[k+1]).T\n",
    "            corr_w1 += np.outer(x, delta_l1)\n",
    "            corr_b1 += (delta_l1 * 1).T\n",
    "            # Backprob, recurrent layer\n",
    "            corr_wh += np.outer(lay1_mem[k], delta_l1) # ! NEW\n",
    "            corr_bh += (delta_l1 * 1).T                 # ! NEW\n",
    "        w2 = w2 - eta * corr_w2\n",
    "        b2 = b2 - eta * corr_b2\n",
    "        w1 = w1 - eta * corr_w1\n",
    "        b1 = b1 - eta * corr_b1\n",
    "        wh = wh - eta * corr_wh # ! NEW\n",
    "        bh = bh - eta * corr_bh # ! NEW\n",
    "        corr_w2 *= 0\n",
    "        corr_b2 *= 0\n",
    "        corr_w1 *= 0\n",
    "        corr_b1 *= 0\n",
    "        corr_wh *= 0\n",
    "        corr_bh *= 0\n",
    "        if n % 1000 == 0:\n",
    "            print(' <', '-' * 3, n)\n",
    "    error[n] /= len(X)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N), error)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
